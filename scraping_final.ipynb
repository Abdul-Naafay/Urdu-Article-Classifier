{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b1de3-ba3d-4270-81e9-af7f54b5897e",
   "metadata": {
    "id": "a38b1de3-ba3d-4270-81e9-af7f54b5897e"
   },
   "outputs": [],
   "source": [
    "%pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f21f967-ba23-447e-abf1-8e740da05e7f",
   "metadata": {
    "id": "3f21f967-ba23-447e-abf1-8e740da05e7f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd583c-16cf-41b0-9ad9-7ff4651e30a4",
   "metadata": {
    "id": "23bd583c-16cf-41b0-9ad9-7ff4651e30a4"
   },
   "source": [
    "# Class Explanation: `NewsScraper`\n",
    "\n",
    "## Overview\n",
    "The `NewsScraper` class is designed for scraping news articles from three different Urdu news websites: Geo, Jang, and Express. The class has methods that cater to each site's unique structure and requirements. Below, we will go through the class and its methods, detailing what each function does, the input it takes, and the output it returns.\n",
    "\n",
    "## Class Definition\n",
    "\n",
    "```python\n",
    "class NewsScraper:\n",
    "    def __init__(self, id_=0):\n",
    "        self.id = id_\n",
    "```\n",
    "\n",
    "\n",
    "## Method 1: `get_express_articles`\n",
    "\n",
    "### Description\n",
    "Scrapes news articles from the Express website across categories like saqafat (entertainment), business, sports, science-technology, and world. The method navigates through multiple pages for each category to gather a more extensive dataset.\n",
    "\n",
    "### Input\n",
    "- **`max_pages`**: The number of pages to scrape for each category (default is 7).\n",
    "\n",
    "### Process\n",
    "- Iterates over each category and page.\n",
    "- Requests each category page and finds article cards within `<ul class='tedit-shortnews listing-page'>`.\n",
    "- Extracts the article's headline, link, and content by navigating through `<div class='horiz-news3-caption'>` and `<span class='story-text'>`.\n",
    "\n",
    "### Output\n",
    "- **Returns**: A tuple of:\n",
    "  - A Pandas DataFrame containing columns: `id`, `title`, and `link`).\n",
    "  - A dictionary `express_contents` where the key is the article ID and the value is the article content.\n",
    "\n",
    "### Data Structure\n",
    "- Article cards are identified by `<li>` tags.\n",
    "- Content is structured within `<span class='story-text'>` and `<p>` tags.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc81de-6bc7-4bde-92e4-1512dcf43aa0",
   "metadata": {
    "id": "d8fc81de-6bc7-4bde-92e4-1512dcf43aa0"
   },
   "outputs": [],
   "source": [
    "class NewsScraper:\n",
    "    def __init__(self,id_=0):\n",
    "        self.id = id_\n",
    "\n",
    "\n",
    "  # write functions to scrape from other websites\n",
    "\n",
    "\n",
    "    def get_express_articles(self, max_pages=1):\n",
    "        express_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "        }\n",
    "        base_url = 'https://www.express.pk'\n",
    "        categories = ['saqafat', 'business', 'sports', 'science', 'world']   # saqafat is entertainment category\n",
    "\n",
    "        # Iterating over the specified number of pages\n",
    "        for category in categories:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}/{category}/archives?page={page}\"\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # Finding article cards\n",
    "                cards = soup.find('ul', class_='tedit-shortnews listing-page').find_all('li')  # Adjust class as per actual site structure\n",
    "                print(f\"\\t--> Found {len(cards)} articles on page {page} of '{category}'.\")\n",
    "\n",
    "                success_count = 0\n",
    "\n",
    "                for card in cards:\n",
    "                    try:\n",
    "                        div = card.find('div',class_='horiz-news3-caption')\n",
    "\n",
    "                        # Article Title\n",
    "                        headline = div.find('a').get_text(strip=True).replace('\\xa0', ' ')\n",
    "\n",
    "                        # Article link\n",
    "                        link = div.find('a')['href']\n",
    "\n",
    "                        # Requesting the content from each article's link\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "                        # Content arranged in paras inside <span> tags\n",
    "                        paras = content_soup.find('span',class_='story-text').find_all('p')\n",
    "\n",
    "                        combined_text = \" \".join(\n",
    "                        p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                        for p in paras if p.get_text(strip=True)\n",
    "                        )\n",
    "\n",
    "                        # Storing data\n",
    "                        express_df['id'].append(self.id)\n",
    "                        express_df['title'].append(headline)\n",
    "                        express_df['link'].append(link)\n",
    "                        express_df['gold_label'].append(category.replace('saqafat','entertainment').replace('science','science-technology'))\n",
    "                        express_df['content'].append(combined_text)\n",
    "\n",
    "                        # Increment ID and success count\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape an article on page {page} of '{category}': {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "            print('')\n",
    "\n",
    "        return pd.DataFrame(express_df)\n",
    "    d def get_express_articles(self, max_pages=7):\n",
    "        express_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "        }\n",
    "        base_url = 'https://www.express.pk'\n",
    "        categories = ['saqafat', 'business', 'sports', 'science', 'world']   # saqafat is entertainment category\n",
    "\n",
    "        # Iterating over the specified number of pages\n",
    "        for category in categories:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}/{category}/archives?page={page}\"\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # Finding article cards\n",
    "                cards = soup.find('ul', class_='tedit-shortnews listing-page').find_all('li')  # Adjust class as per actual site structure\n",
    "                print(f\"\\t--> Found {len(cards)} articles on page {page} of '{category}'.\")\n",
    "\n",
    "                success_count = 0\n",
    "\n",
    "                for card in cards:\n",
    "                    try:\n",
    "                        div = card.find('div',class_='horiz-news3-caption')\n",
    "\n",
    "                        # Article Title\n",
    "                        headline = div.find('a').get_text(strip=True).replace('\\xa0', ' ')\n",
    "\n",
    "                        # Article link\n",
    "                        link = div.find('a')['href']\n",
    "\n",
    "                        # Requesting the content from each article's link\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "                        # Content arranged in paras inside <span> tags\n",
    "                        paras = content_soup.find('span',class_='story-text').find_all('p')\n",
    "\n",
    "                        combined_text = \" \".join(\n",
    "                        p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                        for p in paras if p.get_text(strip=True)\n",
    "                        )\n",
    "\n",
    "                        # Storing data\n",
    "                        express_df['id'].append(self.id)\n",
    "                        express_df['title'].append(headline)\n",
    "                        express_df['link'].append(link)\n",
    "                        express_df['gold_label'].append(category.replace('saqafat','entertainment').replace('science','science-technology'))\n",
    "                        express_df['content'].append(combined_text)\n",
    "\n",
    "                        # Increment ID and success count\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape an article on page {page} of '{category}': {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "            print('')\n",
    "\n",
    "        return pd.DataFrame(express_df)\n",
    "    \n",
    "    def get_jang_articles(self, max_pages=1):\n",
    "        jangnews_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "        }\n",
    "        base_url = 'https://jang.com.pk/category/latest-news'\n",
    "        categories = ['science-and-technology', 'sports', 'entertainment', 'business']  # Adjust as needed\n",
    "\n",
    "        # Iterating over the specified categories\n",
    "        for category in categories:\n",
    "            print(f\"Scraping category '{category}'...\")\n",
    "            url = f\"{base_url}/{category}\"\n",
    "            print(\"Url: \", url)\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                cards = soup.find('ul', class_='scrollPaginationNew__').find_all('li')  # Adjust class as needed\n",
    "                print(f\"\\t--> Found {len(cards)} articles of '{category}'.\")\n",
    "                \n",
    "                success_count = 0\n",
    "                for card in cards:\n",
    "                    try:\n",
    "                        headline_tag = card.find('div', class_='main-heading').find('a')\n",
    "                        headline = headline_tag.get_text(strip=True)\n",
    "                        link = headline_tag['href']\n",
    "\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "                        paras = content_soup.find('div', class_='detail-right').find_all('p')\n",
    "                        combined_text = \" \".join(\n",
    "                            p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                            for p in paras if p.get_text(strip=True)\n",
    "                        )\n",
    "\n",
    "                        jangnews_df['id'].append(self.id)\n",
    "                        jangnews_df['title'].append(headline)\n",
    "                        jangnews_df['link'].append(link)\n",
    "                        jangnews_df['gold_label'].append(category)\n",
    "                        jangnews_df['content'].append(combined_text)\n",
    "\n",
    "                        # Increment ID and success count\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape an article of '{category}': {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles of '{category}'.\")\n",
    "                print('')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to retrieve the category page for '{category}': {e}\")\n",
    "\n",
    "        return pd.DataFrame(jangnews_df)\n",
    "    \n",
    "    def get_duniya(self, max_pages=7):\n",
    "        dunya_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "        }\n",
    "        base_url = 'https://urdu.dunyanews.tv/index.php/ur'\n",
    "        categories = ['Entertainment', 'Business', 'Sports', 'Technology', 'World']   # saqafat is entertainment category\n",
    "\n",
    "        # Iterating over the specified number of pages\n",
    "        for category in categories:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}/{category}\"\n",
    "                print(url)\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # Finding article cards\n",
    "                cards = soup.find('div', class_='newsBox categories').find_all('div')  # Adjust class as per actual site structure\n",
    "                print(f\"\\t--> Found {len(cards)} articles on page {page} of '{category}'.\")\n",
    "\n",
    "                success_count = 0\n",
    "\n",
    "                for card in cards:\n",
    "                    try:\n",
    "                        \n",
    "                        div = card.find('div',class_='col-md-4')\n",
    "                        \n",
    "                        # Article Title\n",
    "                        headline = div.find('a').get_text(strip=True).replace('\\xa0', ' ')\n",
    "                        \n",
    "                        # Article link\n",
    "                        base_url2='https://urdu.dunyanews.tv'\n",
    "                        link = base_url2+div.find('a')['href']\n",
    "                    \n",
    "\n",
    "                        # Requesting the content from each article's link\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "                       # print(content_soup)\n",
    "                        \n",
    "\n",
    "                        # Content arranged in paras inside <span> tags\n",
    "                        paras = content_soup.find('div',class_='main-news col-md-12').find_all('p')\n",
    "                        \n",
    "                        combined_text = \" \".join(\n",
    "                        p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                        for p in paras if p.get_text(strip=True)\n",
    "                        )\n",
    "                        print(combined_text)\n",
    "\n",
    "                        # Storing data\n",
    "                        dunya_df['id'].append(self.id)\n",
    "                        dunya_df['title'].append(headline)\n",
    "                        dunya_df['link'].append(link)\n",
    "                        dunya_df['gold_label'].append(category.replace('Entertainment','entertainment').replace('Technology','science-technology'))\n",
    "                        dunya_df['content'].append(combined_text)\n",
    "\n",
    "                        # Increment ID and success count\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape an article on page {page} of '{category}': {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "            print('')\n",
    "\n",
    "\n",
    "        return pd.DataFrame(dunya_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9a8ad94-10b0-4458-bb7f-3402eecd80d1",
   "metadata": {
    "id": "e9a8ad94-10b0-4458-bb7f-3402eecd80d1"
   },
   "outputs": [],
   "source": [
    "scraper = NewsScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321373e7-8ef4-468f-81d0-8be61fe2ba85",
   "metadata": {
    "id": "321373e7-8ef4-468f-81d0-8be61fe2ba85"
   },
   "outputs": [],
   "source": [
    "express_df = scraper.get_express_articles(max_pages=9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nn7TyroayZhg",
   "metadata": {
    "id": "nn7TyroayZhg"
   },
   "source": [
    "# Output\n",
    "- Save a combined csv of all 3 sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c280426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(express_df['content']) # to remove = numbers , urls , remove empty articles , english , commas (punctuation) , \n",
    "print(express_df) #350\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d1efe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df = scraper.get_geo_articles(max_pages=7) #300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa0fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(geo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9613ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhang_df  = scraper.get_jang_articles() #310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ecd0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jhang_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b69053",
   "metadata": {},
   "outputs": [],
   "source": [
    "duniya_df = scraper.get_duniya(max_pages=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ee2d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(duniya_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5d8a086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "def generate_dates(start_date, num_days):\n",
    "    \"\"\"\n",
    "    Generate a list of incremental dates in the format 'YYYY-MM-DD'.\n",
    "\n",
    "    Args:\n",
    "        start_date (str): The starting date in 'YYYY-MM-DD' format.\n",
    "        num_days (int): Number of dates to generate.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of strings representing incremental dates.\n",
    "    \"\"\"\n",
    "    date_list = []\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    for i in range(num_days):\n",
    "        current_date = start + timedelta(days=i)\n",
    "        date_list.append(current_date.strftime(\"%Y-%m-%d\"))\n",
    "    return date_list\n",
    "\n",
    "# Example usage\n",
    "dates = generate_dates(\"2024-08-01\", 30)\n",
    "# print(type(dates[0]))\n",
    "\n",
    "class News:\n",
    "    def __init__(self,id_=0):\n",
    "        self.id = id_\n",
    "\n",
    "\n",
    "  # write functions to scrape from other websites\n",
    "\n",
    "\n",
    "    def get(self, max_pages=7):\n",
    "        express_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "        }\n",
    "        base_url = 'https://www.dawnnews.tv'\n",
    "        categories = ['life-style', 'business', 'sport', 'tech', 'world']   # saqafat is entertainment category\n",
    "        og_categories = ['entertainment','business','sports','science-technology','world']\n",
    "\n",
    "        # dates = []\n",
    "        # Iterating over the specified number of pages\n",
    "        print(\"here\\n\")\n",
    "        for i,category in enumerate(categories):\n",
    "            # print(\"here\\n\")\n",
    "            for j,date in enumerate(dates):\n",
    "                # print(\"here2\\n\")\n",
    "                print(f\"Scraping page {j} of category '{category}'...\")\n",
    "                url = f\"{base_url}/{category}/{date}/\"\n",
    "                print(f\"{url}\\n\")\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # Finding article cards\n",
    "                articles = soup.find_all('article', class_='bg-white')  # Adjusted for the given structure\n",
    "                print(f\"--> Found {len(articles)} articles on page {j}.\")\n",
    "\n",
    "                success_count = 0\n",
    "\n",
    "                for article in articles:\n",
    "                    try:\n",
    "                        # Extract the title in Urdu\n",
    "                        title_tag = article.find('h2', class_='story__title')\n",
    "                        title_urdu = title_tag.get_text(strip=True)\n",
    "\n",
    "                        # Extract the article link\n",
    "                        link_tag = title_tag.find('a')\n",
    "                        article_link = link_tag['href']\n",
    "\n",
    "                        # Requesting content from the article's link\n",
    "                        article_response = requests.get(article_link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "                        content_container = content_soup.find('div', class_='story__content')\n",
    "\n",
    "# Find all paragraphs and stop before <hr>\n",
    "                        paras = content_container.find_all('p')\n",
    "                        content = []\n",
    "                        for p in paras:\n",
    "                            # Stop if <hr> is encountered\n",
    "                            if p.find_previous_sibling('hr'):\n",
    "                                break\n",
    "                            # Extract and clean text\n",
    "                            text = p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                            if text:\n",
    "                                content.append(text)\n",
    "\n",
    "                        # Combine the paragraphs into a single string\n",
    "                        combined_text = \" \".join(content)\n",
    "\n",
    "                        # Pass the content_soup for further processing (specific to the article page's structure)\n",
    "                        # This part will depend on the additional HTML you provide for the article content page\n",
    "\n",
    "                        # Storing data\n",
    "                        express_df['id'].append(self.id)\n",
    "                        express_df['title'].append(title_urdu)\n",
    "                        express_df['link'].append(article_link)\n",
    "                        express_df['gold_label'].append(og_categories[i])\n",
    "                        express_df['content'].append(combined_text)  # Placeholder until the article's content logic is implemented\n",
    "\n",
    "                        # Increment ID and success count\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"--> Failed to scrape an article: {e}\")\n",
    "\n",
    "                # print(f\"--> Successfully scraped {success_count} articles.\")\n",
    "                        # Content arranged in paras inside <span> tags\n",
    "                        \n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {j} of '{category}'.\")\n",
    "            print('')\n",
    "\n",
    "        return pd.DataFrame(express_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08594ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "scraper = News()\n",
    "dawn = scraper.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9feaf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba5ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Assuming df1, df2, df3, and df4 are your original DataFrames\n",
    "# Filter the DataFrames to include only 'title', 'link', 'content', and 'gold_label'\n",
    "df1_filtered = express_df[['title', 'link', 'content', 'gold_label']]\n",
    "df2_filtered = geo_df[['title', 'link', 'content', 'gold_label']]\n",
    "df3_filtered = jhang_df[['title', 'link', 'content', 'gold_label']]\n",
    "df4_filtered = duniya_df[['title', 'link', 'content', 'gold_label']]\n",
    "df5_filtered = dawn[['title', 'link', 'content', 'gold_label']]\n",
    "\n",
    "\n",
    "# Combine the filtered DataFrames\n",
    "combined_df = pd.concat([df1_filtered, df2_filtered, df3_filtered, df4_filtered,df5_filtered], ignore_index=True)\n",
    "\n",
    "# Remove duplicates based on the 'content' column\n",
    "combined_df = combined_df.drop_duplicates(subset='content', keep='first')\n",
    "\n",
    "# Add a sequential 'id' column\n",
    "combined_df.insert(0, 'id', range(1, len(combined_df) + 1))\n",
    "\n",
    "# Data Cleaning: Remove rows with missing values\n",
    "combined_df = combined_df.dropna()\n",
    "# Define a mapping for standardizing gold labels\n",
    "label_mapping = {\n",
    "    'entertainment': 'entertainment',\n",
    "    'sports': 'sports',\n",
    "    'business': 'business',\n",
    "    'science-and-technology': 'science-technology',\n",
    "    'science-technology': 'science-technology',\n",
    "    'world': 'world',\n",
    "    'Business': 'business',  # Normalize capitalization\n",
    "    'Sports': 'sports'      # Normalize capitalization\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'gold_label' column\n",
    "combined_df['gold_label'] = combined_df['gold_label'].map(label_mapping)\n",
    "\n",
    "# Check for any remaining unmapped labels\n",
    "remaining_labels = combined_df['gold_label'].unique()\n",
    "print(\"Remaining unmapped labels:\", remaining_labels)\n",
    "\n",
    "# Drop rows with unmapped labels if any exist\n",
    "combined_df = combined_df.dropna(subset=['gold_label'])\n",
    "\n",
    "# Get the value counts for the cleaned 'gold_label' column\n",
    "value_counts = combined_df['gold_label'].value_counts()\n",
    "\n",
    "# Print the cleaned value counts\n",
    "print(\"\\nCleaned Value Counts for 'gold_label':\")\n",
    "print(value_counts)\n",
    "\n",
    "\n",
    "# Clean text data in the 'content' column\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespaces\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    return text.strip()\n",
    "\n",
    "combined_df['content'] = combined_df['content'].apply(clean_text)\n",
    "\n",
    "# Ensure 'gold_label' is categorical\n",
    "combined_df['gold_label'] = combined_df['gold_label'].astype('category')\n",
    "\n",
    "# EDA: Print basic statistics and value counts\n",
    "print(\"Dataset Overview:\")\n",
    "print(combined_df.info())\n",
    "print(\"\\nValue Counts for 'gold_label':\")\n",
    "print(combined_df['gold_label'].value_counts())\n",
    "\n",
    "# Plot distribution of labels\n",
    "combined_df['gold_label'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Gold Labels')\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Analyze and visualize content lengths\n",
    "combined_df['content_length'] = combined_df['content'].apply(len)\n",
    "\n",
    "# Plot histogram for content lengths\n",
    "combined_df['content_length'].hist(bins=50)\n",
    "plt.title('Distribution of Content Lengths')\n",
    "plt.xlabel('Content Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot for content lengths\n",
    "plt.boxplot(combined_df['content_length'])\n",
    "plt.title('Boxplot of Content Length')\n",
    "plt.show()\n",
    "\n",
    "# Remove rows with extremely high content lengths (e.g., top 1% as outliers)\n",
    "threshold = combined_df['content_length'].quantile(0.99)\n",
    "combined_df = combined_df[combined_df['content_length'] < threshold]\n",
    "\n",
    "# Save the cleaned dataset to a CSV file\n",
    "combined_df.to_csv('combined_gollum_cleaned.csv', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aefec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.read_csv(\"preprocessed_dataset.csv\")\n",
    "import pandas as pd\n",
    "\n",
    "# Load the newly created df5_filtered\n",
    "df5_filtered = dawn[['title', 'link', 'content', 'gold_label']]\n",
    "\n",
    "# Clean the new dataframe to ensure it matches the format of the existing CSV\n",
    "# Remove duplicates based on the 'content' column\n",
    "df5_filtered = df5_filtered.drop_duplicates(subset='content', keep='first')\n",
    "\n",
    "# Clean text data in the 'content' column\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespaces\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    return text.strip()\n",
    "\n",
    "df5_filtered['content'] = df5_filtered['content'].apply(clean_text)\n",
    "\n",
    "# Define the mapping for standardizing gold labels\n",
    "label_mapping = {\n",
    "    'entertainment': 'entertainment',\n",
    "    'sports': 'sports',\n",
    "    'business': 'business',\n",
    "    'science-and-technology': 'science-technology',\n",
    "    'science-technology': 'science-technology',\n",
    "    'world': 'world',\n",
    "    'Business': 'business',  # Normalize capitalization\n",
    "    'Sports': 'sports'      # Normalize capitalization\n",
    "}\n",
    "\n",
    "df5_filtered['gold_label'] = df5_filtered['gold_label'].map(label_mapping)\n",
    "\n",
    "# Remove rows with missing or unmapped labels\n",
    "df5_filtered = df5_filtered.dropna(subset=['gold_label'])\n",
    "\n",
    "# Add sequential 'id' starting from the max id in the CSV file\n",
    "csv_file_path = 'combined_gollum_cleaned.csv'\n",
    "\n",
    "# Read the existing CSV to find the max id (load only necessary columns)\n",
    "existing_ids = pd.read_csv(csv_file_path, usecols=['id'])\n",
    "max_id = existing_ids['id'].max() if not existing_ids.empty else 0\n",
    "\n",
    "# Assign new IDs to df5_filtered\n",
    "df5_filtered.insert(0, 'id', range(max_id + 1, max_id + 1 + len(df5_filtered)))\n",
    "\n",
    "# Append the new data to the existing CSV file\n",
    "df5_filtered.to_csv(csv_file_path, mode='a', header=False, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Added {len(df5_filtered)} new rows to 'C:\\\\Users\\\\pc\\\\OneDrive\\\\Desktop\\\\proj\\\\combined_gollum_cleaned.csv'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e561d7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('combined_gollum_cleaned.csv')\n",
    "\n",
    "with open('stopwords-ur.txt' , 'r', encoding='utf-8') as file:\n",
    "    stopwords = set(file.read().splitlines())\n",
    "\n",
    "\n",
    "def remove_stopwords(text, stopwords):\n",
    "    words = re.findall(r'\\b\\w+\\b', text)  \n",
    "    filtered_words = [word for word in words if word not in stopwords]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "dataset['content'] = dataset['content'].apply(lambda x: remove_stopwords(str(x), stopwords))\n",
    "dataset.to_csv('preprocessed_dataset.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Saved in new file 'preprocessed_dataset.csv'. Check your folder eelian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ca656b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef37306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "dataset = pd.read_csv(\"preprocessed_dataset.csv\")\n",
    "\n",
    "# Check if the necessary column exists\n",
    "if 'gold_label' in dataset.columns:\n",
    "    # Count the number of articles in each category\n",
    "    category_counts = dataset['gold_label'].value_counts()\n",
    "\n",
    "    # Print the counts\n",
    "    print(\"Number of articles in each category:\")\n",
    "    print(category_counts)\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    category_counts.plot(kind='bar')\n",
    "    plt.title(\"Number of Articles per Category\")\n",
    "    plt.xlabel(\"Category\")\n",
    "    plt.ylabel(\"Number of Articles\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"The column 'gold_label' is missing in the dataset. Please check the file.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
