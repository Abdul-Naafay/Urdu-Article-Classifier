{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.utils.data import random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('preprocessed_dataset.csv')\n",
    "dataset = dataset.dropna(subset=['content'])\n",
    "dataset['content'] = dataset['content'].astype(str)\n",
    "\n",
    "dataset['title'] = dataset['title'].fillna(\"\").astype(str)\n",
    "dataset['combined'] = dataset['title'] + ' ' + dataset['content']\n",
    "\n",
    "X = dataset['combined']\n",
    "\n",
    "y = dataset['gold_label']\n",
    "# y = pd.get_dummies(y, prefix='', prefix_sep='').astype(int)\n",
    "# y = np.array(y)\n",
    "print(y)\n",
    "# print(y.head())\n",
    "# print(X)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_bow = vectorizer.fit_transform(X).toarray()\n",
    "print(X_bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.3, random_state=40)\n",
    "# X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size=0.15/0.85,random_state=40)\n",
    "# print(y_train)\n",
    "# print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionManual:\n",
    "    def __init__(self, learning_rate, epochs, reg_lambda):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.losses = []\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    def cross_entropy_loss(self, y_true, y_pred):\n",
    "        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        reg_loss = (self.reg_lambda / (2 * len(y_true))) * np.sum(self.weights ** 2)\n",
    "        return loss + reg_loss\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.weights = np.zeros(n)\n",
    "        self.bias = 0\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self.sigmoid(linear_model)\n",
    "            \n",
    "            loss = self.cross_entropy_loss(y, y_pred)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            dw = (1 / m) * np.dot(X.T, (y_pred - y)) + (self.reg_lambda / m) * self.weights\n",
    "            db = (1 / m) * np.sum(y_pred - y)\n",
    "            \n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        probs = self.sigmoid(linear_model)  \n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "        return preds, probs\n",
    "    \n",
    "    def evaluate(self,y_true,y_pred):\n",
    "        misclassified_indices = np.where(y_true != y_pred)[0]\n",
    "        print(misclassified_indices)\n",
    "        # print(\"smd\")\n",
    "        accuracy = accuracy_score(y_true,y_pred)\n",
    "        f1 = f1_score(y_true,y_pred)\n",
    "        cm = confusion_matrix(y_true,y_pred)\n",
    "        return accuracy,f1,cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-vs-Rest Classifiers\n",
    "arr = [\"entertainment\",\"business\",\"world\",\"sports\",\"science-technology\"]\n",
    "\n",
    "classifiers = {}\n",
    "losses = {}  # To store losses for each classifier\n",
    "# print(y_train.values)\n",
    "for i in range(5):\n",
    "    y_binary = np.array((y_train == arr[i]).astype(int))  # Current positive class, use this while fitting to train data\n",
    "    print(y_binary)\n",
    "    # if i == 1:\n",
    "    #     model = LogisticRegressionManual(learning_rate=0.01, epochs=100, reg_lambda=0.001)\n",
    "    #     # print(y_binary.values)\n",
    "    # else:\n",
    "    model = LogisticRegressionManual(learning_rate=0.01, epochs=750, reg_lambda=0.001)\n",
    "    \n",
    "    classifiers[i] = model       # declare your logistic regression model here \n",
    "    model.fit(X_train, y_binary)  # fit on your training data and store the cost. You will need to pass y_binary along with the train data\n",
    "    losses[i] = classifiers[i].losses            # Save the cost values for plotting\n",
    "    print(f\"Final loss for Class {i}: {losses[i][-1]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(5):\n",
    "    plt.plot(range(len(losses[i])), losses[i], label=f'Classifier for Class {i}')\n",
    "plt.title('Training Loss for Each Classifier (One-vs-All)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'Class': [],\n",
    "    'Probs':[],\n",
    "    'Accuracy': [],\n",
    "    'F1 Score': [],\n",
    "    'Confusion Matrix': []\n",
    "}\n",
    "\n",
    "for i in range(5):  \n",
    "    y_binary = np.array((arr[i] == y_test).astype(int))\n",
    "    preds, probs = classifiers[i].predict(X_test)\n",
    "\n",
    "    accuracy,f1,cm = classifiers[i].evaluate(y_binary,preds)  \n",
    "    results['Class'].append(i)\n",
    "    results['Probs'].append(probs)\n",
    "    results['Accuracy'].append(accuracy)\n",
    "    results['F1 Score'].append(f1)\n",
    "    results['Confusion Matrix'].append(cm)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(5):\n",
    "    plt.plot(range(len(losses[i])), losses[i], label=f'Classifier for Class {i}')\n",
    "plt.title('Training Loss for Each Classifier (One-vs-All)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "results_df.drop('Probs',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_probs = np.column_stack([p for p in results['Probs']])\n",
    "\n",
    "multi_class_pred = np.argmax(combined_probs, axis=1)\n",
    "predicted_labels = [arr[pred] for pred in multi_class_pred]\n",
    "print(\"Predicted Labels:\", predicted_labels)\n",
    "y_test = np.array(y_test)\n",
    "y_new = []\n",
    "# print(y_test)\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == \"sports\":\n",
    "        y_new.append(3)\n",
    "    elif y_test[i] == \"entertainment\":\n",
    "        y_new.append(0)\n",
    "    elif y_test[i] == \"business\":\n",
    "       y_new.append(1)\n",
    "    elif y_test[i] == \"science-technology\":\n",
    "        y_new.append(4) \n",
    "    elif y_test[i] == \"world\":\n",
    "       y_new.append(2)\n",
    "print(y_new)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_f1 = f1_score(y_new, multi_class_pred, average='macro')\n",
    "accuracy = accuracy_score(y_new, multi_class_pred,)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_new, multi_class_pred)\n",
    "print(f\"Macro F1 Score: {macro_f1:.2f}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urdu = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_point = np.array([urdu])\n",
    "test_point = vectorizer.transform(test_point).toarray()\n",
    "print(test_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0: \"entertainment\", 1: \"business\", 2: \"world\",3:\"sports\" ,4:\"science-technology\" }\n",
    "probs=[]\n",
    "for i in range(5):  \n",
    "    y_pred_class, prob = classifiers[i].predict(test_point)    \n",
    "    probs.append(prob)\n",
    "combined_probs = np.column_stack([p for p in probs])\n",
    "multi_class_pred = np.argmax(combined_probs, axis=1)\n",
    "print(\"Prediction:\", labels[multi_class_pred[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "model = LogisticRegression(\n",
    "    multi_class='multinomial',  \n",
    "    solver='lbfgs',           \n",
    "    max_iter=750,              \n",
    "    random_state=40\n",
    ")\n",
    "print(X_train)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   CODE FOR PLOTTING HEATMAP\n",
    "arr = [\"entertainment\",\"business\",\"world\",\"sports\",\"science-technology\"]\n",
    "\n",
    "classifiers = {}\n",
    "losses = {}  \n",
    "accuracies = []\n",
    "lr = [0.1,0.01,0.001]\n",
    "epochs = [500 , 750 ,1000]\n",
    "\n",
    "for learning_rate in lr:\n",
    "    print(\"learning rate  = \", learning_rate)\n",
    "    for epcoh in epochs:\n",
    "        print(\"epochs  = \",epcoh)\n",
    "        for i in range(5):\n",
    "                y_binary = np.array((y_train == arr[i]).astype(int))\n",
    "                print(y_binary)\n",
    "                \n",
    "                model = LogisticRegressionManual(learning_rate=learning_rate, epochs=epcoh, reg_lambda=0.001)\n",
    "                \n",
    "                classifiers[i] = model      \n",
    "                model.fit(X_train, y_binary) \n",
    "                losses[i] = classifiers[i].losses            \n",
    "                print(f\"Final loss for Class {i}: {losses[i][-1]}\")\n",
    "\n",
    "                results = {\n",
    "                'Class': [],\n",
    "                'Probs':[],\n",
    "                'Accuracy': [],\n",
    "                'F1 Score': [],\n",
    "                'Confusion Matrix': []\n",
    "                        }\n",
    "\n",
    "        for i in range(5):  \n",
    "            y_binary = np.array((arr[i] == y_test).astype(int))\n",
    "            preds, probs = classifiers[i].predict(X_test)\n",
    "\n",
    "            accuracy,f1,cm = classifiers[i].evaluate(y_binary,preds)  \n",
    "            results['Class'].append(i)\n",
    "            results['Probs'].append(probs)\n",
    "            results['Accuracy'].append(accuracy)\n",
    "            results['F1 Score'].append(f1)\n",
    "            results['Confusion Matrix'].append(cm)\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.drop('Probs',axis=1)\n",
    "        combined_probs = np.column_stack([p for p in results['Probs']])\n",
    "\n",
    "        multi_class_pred = np.argmax(combined_probs, axis=1)\n",
    "        predicted_labels = [arr[pred] for pred in multi_class_pred]\n",
    "        print(\"Predicted Labels:\", predicted_labels)\n",
    "        y_test = np.array(y_test)\n",
    "        y_new = []\n",
    "      \n",
    "        for i in range(len(y_test)):\n",
    "            if y_test[i] == \"sports\":\n",
    "                y_new.append(3)\n",
    "            elif y_test[i] == \"entertainment\":\n",
    "                y_new.append(0)\n",
    "            elif y_test[i] == \"business\":\n",
    "                y_new.append(1)\n",
    "            elif y_test[i] == \"science-technology\":\n",
    "                y_new.append(4) \n",
    "            elif y_test[i] == \"world\":\n",
    "                 y_new.append(2)\n",
    "        print(y_new)\n",
    "\n",
    "\n",
    "        macro_f1 = f1_score(y_new, multi_class_pred, average='macro')\n",
    "        accuracy = accuracy_score(y_new, multi_class_pred,)\n",
    "        accuracies.append(accuracy)\n",
    "        print(\"acc = \", accuracy)\n",
    "\n",
    "        conf_matrix = confusion_matrix(y_new, multi_class_pred)\n",
    "\n",
    "        print(f\"Macro F1 Score: {macro_f1:.2f}\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(conf_matrix)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "learning_rates = lr = [0.1,0.01,0.001]\n",
    "\n",
    "epochs = [500, 750, 1000]  \n",
    "accuracies = accuracies  \n",
    "\n",
    "accuracy_matrix = np.array(accuracies).reshape(len(learning_rates), len(epochs))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "heatmap = plt.imshow(accuracy_matrix, aspect='auto')\n",
    "\n",
    "plt.colorbar(heatmap, label='Accuracy')\n",
    "\n",
    "plt.xticks(range(len(epochs)), labels=epochs, fontsize=10)\n",
    "plt.yticks(range(len(learning_rates)), labels=learning_rates, fontsize=10)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Learning Rate', fontsize=12)\n",
    "plt.title('Accuracy Heatmap', fontsize=14)\n",
    "\n",
    "for i in range(len(learning_rates)):\n",
    "    for j in range(len(epochs)):\n",
    "        plt.text(j, i, f\"{accuracy_matrix[i, j]:.2f}\", ha='center', va='center', color='black', fontsize=10)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
