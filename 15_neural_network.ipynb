{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./preprocessed_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"rtatman/urdu-stopwords-list\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "stop_words=pd.read_json(\"/Users/abdullahasif/Documents/University/Sem_5/ml/GroupNo_Project/stopwords-ur.json.txt\")\n",
    "stop_words=stop_words[0].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# # Define a list of Urdu stop words (you can expand this list)\n",
    "# # stop_words = {\"ہے\", \"ہیں\", \"کو\", \"کے\", \"کا\", \"میں\", \"اور\", \"سے\", \"یہ\", \"وہ\", \"پر\", \"تو\", \"ایک\"}\n",
    "\n",
    "# # Initialize a Counter to store the bag of words\n",
    "bag_of_words = Counter()\n",
    "\n",
    "# Regex pattern to remove punctuations, numbers, and English words\n",
    "pattern = re.compile(r\"[^\\u0600-\\u06FF\\s]\")  # Only keep Urdu characters and spaces\n",
    "for i, content in enumerate(df[\"content\"]):\n",
    "    if i != 124:  # Skip the row with index 124\n",
    "        # Remove punctuations, numbers, and English words\n",
    "        cleaned_content = pattern.sub(\"\", content)\n",
    "        \n",
    "        # Tokenize and filter out stop words\n",
    "        tokens = [word for word in cleaned_content.split() if word not in stop_words]\n",
    "        \n",
    "        # Update the Counter with filtered tokens\n",
    "        bag_of_words.update(tokens)\n",
    "\n",
    "# Convert the Counter to a dictionary (optional, if needed)\n",
    "bag_of_words_list = list(bag_of_words)\n",
    "bag_of_words_dict = {word:i for i,word in enumerate(bag_of_words_list)}\n",
    "gold_labels=[]\n",
    "labels_dict = list(set(df[\"gold_label\"]))\n",
    "labels_dict = {word:i for i, word in enumerate(labels_dict)}\n",
    "for i,label in enumerate(df[\"gold_label\"]):\n",
    "    if i ==124:\n",
    "        continue\n",
    "    vector = [0]*len(labels_dict)\n",
    "    vector[labels_dict[label]] = 1\n",
    "    gold_labels.append(vector)\n",
    "gold_labels=torch.tensor(gold_labels)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "dataset = pd.read_csv('preprocessed_dataset.csv')\n",
    "dataset = dataset.dropna(subset=['content'])\n",
    "dataset['content'] = dataset['content'].astype(str)\n",
    "\n",
    "X = dataset['content']\n",
    "\n",
    "y = dataset['gold_label']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(X)\n",
    "X_bow=X_bow.toarray()\n",
    "X_bow =torch.Tensor(X_bow)\n",
    "# Display the bag of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# class BagOfWords:\n",
    "#     def __init__(self):\n",
    "#         self.words = bag_of_words_dict\n",
    "    \n",
    "#     def vectorize(self, sentence):\n",
    "#         vector = [0] * len(self.words)\n",
    "#         cleaned_content = pattern.sub(\"\", content)\n",
    "        \n",
    "#         # Tokenize and filter out stop words\n",
    "#         words = [word for word in cleaned_content.split() if word not in stop_words]\n",
    "#         for word in words:\n",
    "#             if word in self.words:\n",
    "#                 vector[self.words[word]] += 1\n",
    "#         return torch.tensor(vector)\n",
    "    \n",
    "#     def transform(self, sentences):\n",
    "#         return torch.stack([self.vectorize(sentence) for i,sentence in enumerate(sentences) if i !=124])\n",
    "# bow = BagOfWords()\n",
    "# input = bow.transform(df[\"content\"])\n",
    "# len(bag_of_words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset,random_split,DataLoader\n",
    "# gold_labels = torch.argmax(gold_labels, dim=1)\n",
    "unique, counts = torch.unique(torch.argmax(gold_labels, dim=1), return_counts=True)\n",
    "dataset =TensorDataset(X_bow.float(),gold_labels.float())\n",
    "tr = int(len(dataset)*0.7)\n",
    "te = int(len(dataset)*0.15)\n",
    "va = len(dataset)-tr-te\n",
    "print(tr,te,va)\n",
    "train, test, val= random_split(dataset,[tr,te,va])\n",
    "train_loader = DataLoader(dataset=train,shuffle=True,batch_size=32)\n",
    "test_loader = DataLoader(dataset=test,shuffle=True,batch_size=32)\n",
    "val_loader = DataLoader(dataset=val,shuffle=True,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the neural network\n",
    "class LinearNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearNN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            # nn.Linear(input_size,128*5),\n",
    "            # nn.LeakyReLU(),\n",
    "            # # nn.Dropout(),\n",
    "            # nn.Linear(128*5,64),\n",
    "            # nn.LeakyReLU(),\n",
    "            # # nn.Dropout(),\n",
    "            # nn.Linear(64,output_size),\n",
    "            # nn.Sigmoid()\n",
    "        nn.Linear(input_size, 256),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(256, output_size)\n",
    "        )\n",
    "\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = torch.flatten(x,start_dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Parameters\n",
    "\n",
    "\n",
    "# Example of dummy input and target\n",
    "# input_tensor = torch.randn(861, input_size)  # Dummy input\n",
    "# target_tensor = torch.randint(0, output_size, (861,))  # Dummy target (class indices)\n",
    "# print(input_tensor.shape)\n",
    "# print(target_tensor.shape)\n",
    "# Training loop (single epoch example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def fit_one_epoch(model, data_loader, optimizer, loss_fn):\n",
    "    '''\n",
    "    Perform one epoch of training\n",
    "    '''\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for (inputs, labels) in data_loader:\n",
    "        outputs = model(inputs)  \n",
    "        loss = loss_fn(outputs, labels.float()) \n",
    "        # print(outputs[:3])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, loss_fn):\n",
    "    '''\n",
    "    Perform one epoch of evaluation\n",
    "    '''\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred = []\n",
    "    true = []\n",
    "    for inputs, labels in data_loader:\n",
    "        outputs = model(inputs)  # Logits output\n",
    "        # probabilities = torch.softmax(outputs,dim=1)  # Apply softmax to logits\n",
    "        loss = loss_fn(outputs, labels.float())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # print(\"Class distribution:\", labels.sum(dim=0))\n",
    "\n",
    "        # predicted = torch.argmax(probabilities, dim=1)\n",
    "        # labels_max = torch.argmax(labels, dim=1)\n",
    "        predicted = torch.argmax(outputs, dim=1)  # Predicted classes\n",
    "        labels_max = torch.argmax(labels, dim=1)  # True labels\n",
    "        # print(predicted[0], labels_max[0]) \n",
    "        pred.extend(predicted)\n",
    "        true.extend(labels_max)\n",
    "        # correct += (predicted == labels_max).sum().item()\n",
    "        # total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    # accuracy = correct / total\n",
    "    return avg_loss, accuracy_score(pred,true)\n",
    "\n",
    "\n",
    "def fit(model, train_loader, val_loader, optimizer, loss_fn, epochs):\n",
    "    '''\n",
    "    Perform the entire training process\n",
    "    '''\n",
    "    history = [list(), list(), list(), list()] \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_accuracy = evaluate(model, train_loader, loss_fn)\n",
    "        train_loss = fit_one_epoch(model, train_loader, optimizer, loss_fn)\n",
    "        val_loss, val_accuracy = evaluate(model, val_loader, loss_fn)\n",
    "\n",
    "        history[0].append(train_loss)\n",
    "        history[1].append(train_accuracy)\n",
    "        history[2].append(val_loss)\n",
    "        history[3].append(val_accuracy)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset,random_split,DataLoader\n",
    "# gold_labels = torch.argmax(gold_labels, dim=1)\n",
    "unique, counts = torch.unique(torch.argmax(gold_labels, dim=1), return_counts=True)\n",
    "dataset =TensorDataset(X_bow.float(),gold_labels.float())\n",
    "tr = int(len(dataset)*0.7)\n",
    "te = int(len(dataset)*0.15)\n",
    "va = len(dataset)-tr-te\n",
    "\n",
    "train, test, val= random_split(dataset,[tr,te,va])\n",
    "train_loader = DataLoader(dataset=train,shuffle=True,batch_size=64)\n",
    "test_loader = DataLoader(dataset=test,shuffle=True,batch_size=64)\n",
    "val_loader = DataLoader(dataset=val,shuffle=True,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "# model = LinearNN(input_size,output_size)\n",
    "input_size = 15800\n",
    "output_size = 5\n",
    "model = LinearNN(input_size, output_size)\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0)\n",
    "history = fit(model,train_loader,val_loader,optimizer,loss_fn,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.no_grad()\n",
    "model.eval()\n",
    "# outputs = []\n",
    "# for inputs, labels in test_loader:        \n",
    "#         outputs.append( model(inputs))  # Logits output\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "y_pred, y_true = [], []\n",
    "for inputs, labels in test_loader:\n",
    "    outputs = model(inputs)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    labels_max = torch.argmax(labels, dim=1)\n",
    "    y_pred.extend(predicted.tolist())\n",
    "    y_true.extend(labels_max.tolist())\n",
    "print(accuracy_score(y_pred,y_true))\n",
    "print(confusion_matrix(y_pred,y_true))\n",
    "print(classification_report(y_pred,y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model,test_loader,loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LinearNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers, dropout_rate=0.0):\n",
    "        super(LinearNN, self).__init__()\n",
    "        layers = []\n",
    "        in_features = input_size\n",
    "        for hidden_units in hidden_layers:\n",
    "            layers.append(nn.Linear(in_features, hidden_units))\n",
    "            layers.append(nn.LeakyReLU())\n",
    "            if dropout_rate > 0:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "            in_features = hidden_units\n",
    "        layers.append(nn.Linear(in_features, output_size))\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Training and evaluation functions\n",
    "def fit_one_epoch(model, data_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in data_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    y_pred, y_true = [], []\n",
    "    for inputs, labels in data_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        labels_max = torch.argmax(labels, dim=1)\n",
    "        y_pred.extend(predicted.tolist())\n",
    "        y_true.extend(labels_max.tolist())\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return total_loss / len(data_loader), accuracy\n",
    "\n",
    "def run_experiment(hyperparams, input_size, output_size, dataset, runs=3):\n",
    "    results = []\n",
    "    for i,config in enumerate(hyperparams):\n",
    "        lr, batch_size, epochs, weight_decay, hidden_layers, dropout_rate = config\n",
    "        train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
    "        for _ in range(runs):  # Multiple runs for randomness\n",
    "            tr, te, va = int(len(dataset) * 0.7), int(len(dataset) * 0.15), len(dataset) - int(len(dataset) * 0.7) - int(len(dataset) * 0.15)\n",
    "            train, test, val = random_split(dataset, [tr, te, va])\n",
    "            train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            model = LinearNN(input_size, output_size, hidden_layers, dropout_rate)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                fit_one_epoch(model, train_loader, optimizer, loss_fn)\n",
    "            val_loss, val_acc = evaluate(model, val_loader, loss_fn)\n",
    "\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_acc)\n",
    "        \n",
    "        avg_val_loss = sum(val_losses) / runs\n",
    "        avg_val_acc = sum(val_accuracies) / runs\n",
    "        results.append((config, avg_val_loss, avg_val_acc))\n",
    "        print(f\"{i+1}'th iteration done out of {len(hyperparams)}\")\n",
    "    return results\n",
    "\n",
    "learning_rates = [0.01,0.001, 0.0001]\n",
    "batch_sizes = [32, 64,128]\n",
    "epochs = [20,40,100]\n",
    "weight_decays = [0.0, 1e-4]\n",
    "hidden_layers_configs = [[256], [256, 128], [512, 256, 128]]\n",
    "dropout_rates = [0.0, 0.2, 0.3]\n",
    "\n",
    "hyperparams = list(itertools.product(learning_rates, batch_sizes, epochs, weight_decays, hidden_layers_configs, dropout_rates))\n",
    "print(len(hyperparams))\n",
    "input_size = 15800\n",
    "output_size = 5\n",
    "dataset = TensorDataset(X_bow.float(), gold_labels.float())\n",
    "results = run_experiment(hyperparams, input_size, output_size, dataset)\n",
    "for config, val_loss, val_acc in results:\n",
    "    print(f\"Config: {config}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "learning_rate_results = {lr: [] for lr in learning_rates}\n",
    "for config, _, val_acc in results:\n",
    "    learning_rate_results[config[0]].append(val_acc)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for lr, accs in learning_rate_results.items():\n",
    "    plt.plot(accs, label=f\"Learning Rate: {lr}\")\n",
    "plt.title(\"Validation Accuracy for Different Learning Rates\")\n",
    "plt.xlabel(\"Experiment\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df1= pd.DataFrame(results, columns=['Config', 'Val Loss', 'Val Accuracy'])\n",
    "results_df1['Config'] = results_df1['Config'].astype(str)\n",
    "results_df1.to_csv('results2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "configs = results_df1[\"Config\"].apply(eval)  \n",
    "losses = results_df1[\"Val Loss\"]\n",
    "accuracies = results_df1[\"Val Accuracy\"]\n",
    "i=accuracies.argmax()\n",
    "print(configs[i],accuracies[i])\n",
    "\n",
    "columns = [\"learning_rate\", \"batch_size\", \"epochs\", \"weight_decay\", \"hidden_layers_config\", \"dropout_rate\"]\n",
    "configs_df = pd.DataFrame(configs.tolist(), columns=columns)\n",
    "configs_df[\"hidden_layers_config\"] = configs_df[\"hidden_layers_config\"].apply(tuple)\n",
    "results_df = pd.concat([configs_df, losses, accuracies], axis=1)\n",
    "results_df.columns = columns + [\"loss\", \"accuracy\"]\n",
    "heatmap_data = results_df.groupby([\"learning_rate\", \"hidden_layers_config\"])[\"accuracy\"].mean().unstack()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap=\"coolwarm\", fmt=\".3f\")\n",
    "plt.title(\"Accuracy Heatmap: Learning Rate vs. Hidden Layers Config\")\n",
    "plt.xlabel(\"Hidden Layers Config\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, te, va = int(len(dataset) * 0.7), int(len(dataset) * 0.15), len(dataset) - int(len(dataset) * 0.7) - int(len(dataset) * 0.15)\n",
    "train, test, val = random_split(dataset, [tr, te, va])\n",
    "train_loader = DataLoader(train, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test, batch_size=128, shuffle=False)\n",
    "\n",
    "model = LinearNN(input_size, output_size, [256], 0.0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(20):\n",
    "    fit_one_epoch(model, train_loader, optimizer, loss_fn)\n",
    "val_loss, val_acc = evaluate(model, val_loader, loss_fn)\n",
    "print(evaluate(model,test_loader,loss_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "class ConfigurableNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers, activation_fn):\n",
    "        super(ConfigurableNN, self).__init__()\n",
    "        layers = []\n",
    "        in_features = input_size\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(in_features, hidden_size))\n",
    "            layers.append(activation_fn())\n",
    "            in_features = hidden_size\n",
    "        layers.append(nn.Linear(in_features, output_size))\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Training and evaluation functions\n",
    "def fit_one_epoch(model, data_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in data_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    y_pred, y_true = [], []\n",
    "    for inputs, labels in data_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels.float())\n",
    "        total_loss += loss.item()\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        labels_max = torch.argmax(labels, dim=1)\n",
    "        y_pred.extend(predicted.tolist())\n",
    "        y_true.extend(labels_max.tolist())\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return total_loss / len(data_loader), accuracy\n",
    "\n",
    "def train_and_evaluate_model(input_size, output_size, hidden_layers, activation_fn, train_loader, val_loader, epochs, lr):\n",
    "    model = ConfigurableNN(input_size, output_size, hidden_layers, activation_fn).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    best_val_accuracy = 0\n",
    "    for _ in range(epochs):\n",
    "        fit_one_epoch(model, train_loader, optimizer, loss_fn)\n",
    "        _, val_accuracy = evaluate(model, val_loader, loss_fn)\n",
    "        best_val_accuracy = max(best_val_accuracy, val_accuracy)\n",
    "    return best_val_accuracy\n",
    "\n",
    "input_size = 15800\n",
    "output_size = 5\n",
    "hidden_layer_options = [[128], [256, 128], [512, 256, 128]]\n",
    "activation_options = [nn.ReLU, nn.LeakyReLU, nn.Tanh, nn.Softmax,nn.Sigmoid]\n",
    "learning_rates = [0.0001]\n",
    "epochs = 100\n",
    "repeats_per_config = 10\n",
    "\n",
    "X_bow, gold_labels = X_bow.to(device), gold_labels.to(device)\n",
    "\n",
    "dataset = TensorDataset(X_bow.float(), gold_labels.float())\n",
    "tr, te, va = int(len(dataset) * 0.7), int(len(dataset) * 0.15), len(dataset) - int(len(dataset) * 0.7) - int(len(dataset) * 0.15)\n",
    "train, test, val = random_split(dataset, [tr, te, va])\n",
    "train_loader = DataLoader(train, shuffle=True, batch_size=64)\n",
    "val_loader = DataLoader(val, shuffle=True, batch_size=64)\n",
    "\n",
    "results = []\n",
    "configs = list(itertools.product(hidden_layer_options, activation_options, learning_rates))\n",
    "print(len(configs))\n",
    "for hidden_layers, activation_fn, lr in configs:\n",
    "    val_accuracies = []\n",
    "    for _ in range(repeats_per_config):\n",
    "        val_accuracy = train_and_evaluate_model(input_size, output_size, hidden_layers, activation_fn, train_loader, val_loader, epochs, lr)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "    avg_accuracy = np.mean(val_accuracies)\n",
    "    results.append((hidden_layers, activation_fn.__name__, lr, avg_accuracy))\n",
    "    print(f\"Config: Hidden Layers={hidden_layers}, Activation={activation_fn.__name__}, LR={lr}, Avg Accuracy={avg_accuracy:.4f}\")\n",
    "hidden_layer_labels = ['-'.join(map(str, layers)) for layers, _, _, _ in results]\n",
    "accuracies = [accuracy for _, _, _, accuracy in results]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(hidden_layer_labels, accuracies, color='skyblue')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Model Performance for Different Configurations')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_df1= pd.DataFrame(results, columns=['Config', 'Activation', 'Lr','val_accuracies'])\n",
    "results_df1['Config'] = results_df1['Config'].astype(str)\n",
    "results_df1.to_csv('results_neural.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df= pd.read_csv(\"./results2.csv\")\n",
    "configs = df[\"Config\"].apply(eval) \n",
    "losses = df[\"Val Loss\"]\n",
    "accuracies = df[\"Val Accuracy\"]\n",
    "i=accuracies.argmax()\n",
    "print(configs[i],accuracies[i])\n",
    "columns = [\"learning_rate\", \"batch_size\", \"epochs\", \"weight_decay\", \"hidden_layers_config\", \"dropout_rate\"]\n",
    "configs_df = pd.DataFrame(configs.tolist(), columns=columns)\n",
    "configs_df[\"hidden_layers_config\"] = configs_df[\"hidden_layers_config\"].apply(tuple)\n",
    "results_df = pd.concat([configs_df, losses, accuracies], axis=1)\n",
    "results_df.columns = columns + [\"loss\", \"accuracy\"]\n",
    "heatmap_data = results_df.groupby([\"learning_rate\", \"hidden_layers_config\"])[\"accuracy\"].mean().unstack()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap=\"coolwarm\", fmt=\".3f\")\n",
    "plt.title(\"Accuracy Heatmap: Learning Rate vs. Hidden Layers Config\")\n",
    "plt.xlabel(\"Hidden Layers Config\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "for col in ['learning_rate', 'batch_size', 'epochs', 'weight_decay', \n",
    "            'hidden_layers_config', 'dropout_rate']:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.lineplot(data=results_df, x=col, y='accuracy', marker='o')\n",
    "    plt.title(f'Accuracy vs. {col}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = ['learning_rate', 'batch_size', 'epochs', \n",
    "               'weight_decay', 'hidden_layers_config', 'dropout_rate']\n",
    "results_df['hidden_layers_config'] = results_df['hidden_layers_config'].astype(str)\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 18))\n",
    "axes = axes.flatten()\n",
    "colors = sns.color_palette(\"husl\", len(hyperparams))\n",
    "# Generate boxplots\n",
    "for ax, param,col in zip(axes, hyperparams,colors):\n",
    "    sns.boxplot(data=results_df, x=param, y='accuracy', ax=ax,color=col)\n",
    "    ax.set_title(f'Accuracy Distribution by {param}')\n",
    "    ax.set_xlabel(param)\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pivot_table = results_df.pivot_table(values='accuracy', \n",
    "                             index='learning_rate', \n",
    "                             columns='batch_size')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(pivot_table, annot=True, cmap='YlGnBu', fmt=\".5f\")\n",
    "plt.title('Accuracy Heatmap: Learning Rate vs. Batch Size')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
